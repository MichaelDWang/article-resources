{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf # require historical Tensorflow GPU version before 2.0 and Cuda packages.\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import jieba\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(text_str):\n",
    "    # stopwords\n",
    "    stopword_dir = \"Stopwords_Chinese.txt\"\n",
    "    sw_list = []\n",
    "    f = open(stopword_dir, \"r\", encoding='utf-8-sig').read().splitlines()\n",
    "    for word in f:\n",
    "        sw_list.append(word)\n",
    "    text = unicodedata.normalize('NFKC', text_str)\n",
    "    seg_list = jieba.cut(text, cut_all = False, HMM = True)\n",
    "    seg_list = [word.lower() for word in seg_list if word not in sw_list # remove stopwords\n",
    "                and word not in string.punctuation # remove punctuation\n",
    "                and not word.isnumeric() # remove digits\n",
    "                and word not in ['\\ue5e5',' ']] \n",
    "    text1 = \" \".join(seg_list)\n",
    "    return text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_learning_classifier\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.layers import LSTM\n",
    "from keras import optimizers\n",
    "from keras import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Set data directory\n",
    "save_wordindex = 'trained models/model1/binary_rnn/tokenizer.pickle'\n",
    "save_final_model1 = 'trained models/model1/binary_rnn/save_final_model.h5'\n",
    "save_final_model2 = 'trained models/model2/binary_rnn/save_final_model.h5'\n",
    "\n",
    "# Load model\n",
    "import os\n",
    "import tensorflow as tf\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR) # To ignore keep_dims warning\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Choose default GPU for computing\n",
    "model1 = load_model(save_final_model1)\n",
    "model2 = load_model(save_final_model2)\n",
    "\n",
    "# loading tokenizer\n",
    "with open(save_wordindex, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "def deep_learning_classifier(content_string):\n",
    "    # Set parameters\n",
    "    maxlen = 50\n",
    "    # text\n",
    "    encoded_text = tokenizer.texts_to_sequences([content_string])\n",
    "    x_predict = np.asarray(preprocessing.sequence.pad_sequences(encoded_text, maxlen = maxlen))\n",
    "    label1_array = model1.predict(x_predict)\n",
    "    label2_array = model2.predict(x_predict)\n",
    "    a = label1_array[0]\n",
    "    b = label2_array[0]\n",
    "    # output binary result\n",
    "    label1_array = np.array([0,0])\n",
    "    label2_array = np.array([0,0])\n",
    "    label1_array[np.argmax(a)]=1\n",
    "    label2_array[np.argmax(b)]=1\n",
    "    return label1_array, label2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary_sentiment_classifier\n",
    "\n",
    "# Choose dictionary to create sentiment lists\n",
    "dictionaries = ['NTUSD', 'Tsinghua_lijun']\n",
    "sentiment = ['positive.txt','negative.txt']\n",
    "\n",
    "# Positive\n",
    "pos_list = []\n",
    "for d in dictionaries:\n",
    "    inputdir = 'sentiment dictionary/' + d + '/positive.txt'\n",
    "    with open(inputdir, 'r', encoding = 'utf-8-sig') as f:\n",
    "        print('Loading positive dictionary: %s' %d)\n",
    "        for word in f:\n",
    "            if word.strip() not in pos_list:\n",
    "                pos_list.append(word.strip())\n",
    "# Negative\n",
    "neg_list = []\n",
    "for d in dictionaries:\n",
    "    inputdir = 'sentiment dictionary/' + d + '/negative.txt'\n",
    "    with open(inputdir, 'r', encoding = 'utf-8-sig') as f:\n",
    "        print('Loading negative dictionary: %s' %d)\n",
    "        for word in f:\n",
    "            if word.strip() not in pos_list:\n",
    "                neg_list.append(word.strip())\n",
    "print('Completed!')\n",
    "\n",
    "# Create classifier function\n",
    "def dict_classifier(content_string):\n",
    "    text_data = content_string.split()\n",
    "    pos_words = [x for x in text_data if x in pos_list]\n",
    "    neg_words = [x for x in text_data if x in neg_list]\n",
    "    label1_array = np.array([0,0])\n",
    "    label2_array = np.array([0,0])\n",
    "    if pos_words > neg_words:\n",
    "        label2_array = np.array([0,1])\n",
    "    if pos_words < neg_words:\n",
    "        label2_array = np.array([1,0])\n",
    "    return label1_array,label2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined classifier\n",
    "from keras import preprocessing\n",
    "\n",
    "# Set weights of classifiers\n",
    "dl = 1\n",
    "svm = 1\n",
    "dic = 1\n",
    "dl_label1_acc = 1\n",
    "dl_label2_acc = 1\n",
    "svm_label1_acc = 1\n",
    "svm_label2_acc = 1\n",
    "dic_label2_acc = 1\n",
    "\n",
    "def combined_classifier(topic_content_list,topic_list):\n",
    "    label1 = []\n",
    "    label2 = []\n",
    "    i = 1\n",
    "    for num in range(0,len(topic_content_list)):\n",
    "        label1_array = 0\n",
    "        label2_array = 0\n",
    "        # RNN classifier result\n",
    "        result1 = deep_learning_classifier(topic_content_list[num])\n",
    "        label1_array = dl*dl_label1_acc * result1[0]\n",
    "        label2_array = dl*dl_label2_acc * result1[1]\n",
    "        # SVM classifier result\n",
    "        result2 = svm_classifier(topic_list[num])\n",
    "        label1_array += svm*svm_label1_acc * result2[0]\n",
    "        label2_array += svm*svm_label2_acc * result2[1]\n",
    "        # Dictionary classifier result\n",
    "        result1 = dict_classifier(topic_content_list[num])\n",
    "        label2_array += dic*dic_label2_acc * result1[1]\n",
    "        # predict\n",
    "        label1.append(np.argmax(label1_array))\n",
    "        label2.append(np.argmax(label2_array))\n",
    "        if i % 200 == 0:\n",
    "            print(i,'out of ', len(topic_content_list))\n",
    "        i += 1\n",
    "    return label1,label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy in training set\n",
    "from sklearn.metrics import accuracy_score\n",
    "import jieba\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def test_data(seed,test_size):\n",
    "    \n",
    "    test_dir = \"data/news corpus/subset_2650.csv\"\n",
    "    tdf = pd.read_csv(test_dir, sep=',', quotechar='\"', encoding=\"utf8\").sample(frac=1, random_state = seed)\n",
    "\n",
    "    test_sublist_1 = []\n",
    "    test_sublist_2 = []\n",
    "    test_sublabels_1 = []\n",
    "    test_sublabels_2 = []\n",
    "\n",
    "    for index, row in tdf.iterrows():\n",
    "        title = row['title']\n",
    "        content = row['content']\n",
    "\n",
    "        if index >= len(tdf) * (1 - test_size):\n",
    "\n",
    "            # binary label\n",
    "            old_label1 = tdf[\"topic_country\"].iloc[index]\n",
    "            old_label2 = tdf[\"sentiment\"].iloc[index]\n",
    "\n",
    "            if old_label1 == 4:\n",
    "                new_label1 = 0\n",
    "            else:\n",
    "                new_label1 = 1\n",
    "            if old_label2 == 1 or old_label2 == 2:\n",
    "                new_label2 = 1\n",
    "            else:\n",
    "                new_label2 = 0\n",
    "\n",
    "            test_sublist_1.append(cut(title + content)) # Record \"text\" column to train_list\n",
    "            test_sublist_2.append(cut(title))\n",
    "            test_sublabels_1.append(new_label1) # Record \"label_1\" column to train_labels\n",
    "            test_sublabels_2.append(new_label2) # Record \"label_2\" column to train_labels\n",
    "    test_labels_1_array = np.array(test_sublabels_1)\n",
    "    test_labels_2_array = np.array(test_sublabels_2)\n",
    "\n",
    "    return test_sublist_1,test_sublist_2,test_labels_1_array,test_labels_2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = []\n",
    "loss_list = []\n",
    "\n",
    "for seed in [34]:\n",
    "    results = test_data(seed,1)\n",
    "    test_sublist_1 = results[0]\n",
    "    test_sublist_2 = results[1]\n",
    "    test_labels_1_array = results[2]\n",
    "    test_labels_2_array = results[3]\n",
    "    output = combined_classifier(test_sublist_1,test_sublist_2)\n",
    "    label1 = output[0]\n",
    "    label2 = output[1]\n",
    "    # Test accuracy\n",
    "    acc1 = accuracy_score(test_labels_1_array, np.array(label1))\n",
    "    print('\\nTesting accuracy of Topic label is: %.2f' % acc1)\n",
    "    acc2 = accuracy_score(test_labels_2_array, np.array(label2))\n",
    "    print('\\nTesting accuracy of Sentiment label is: %.2f' % acc2)\n",
    "    loss = (1-acc1) + (1-acc2)**2\n",
    "    \n",
    "    seed_list.append(seed)\n",
    "    loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = combined_classifier(test_sublist_1,test_sublist_2)\n",
    "label1 = results[0]\n",
    "label2 = results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "acc1 = accuracy_score(test_labels_1_array, np.array(label1))\n",
    "print('\\nTesting accuracy of Topic label is: %.2f' % acc1)\n",
    "acc2 = accuracy_score(test_labels_2_array, np.array(label2))\n",
    "print('\\nTesting accuracy of Sentiment label is: %.2f' % acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classifier to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Set data directory\n",
    "input_dir = \"data/news corpus/US_contents_clean.csv\"\n",
    "output_dir = \"data/annotated/annotated_contents.csv\"\n",
    "        \n",
    "# Load data\n",
    "df = pd.read_csv(input_dir, sep=',', quotechar='\"', encoding=\"utf-8-sig\")\n",
    "df = df.dropna(how='any') # drop row with any missing value\n",
    "df = df.sort_values(by = 'created_at')\n",
    "df.reset_index(inplace=True) # reset index after sorting\n",
    "del df['index'] # delete old index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing list and loading data\n",
    "pred_sublist_1 = []\n",
    "pred_sublist_2 = []\n",
    "pred_sublabels_1 = []\n",
    "pred_sublabels_2 = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # change full size to half size characters\n",
    "    title = row['title']\n",
    "    content = row['content']\n",
    "\n",
    "    pred_sublist_1.append(cut(title + content))\n",
    "    pred_sublist_2.append(cut(title))\n",
    "    \n",
    "results = combined_classifier(pred_sublist_1,pred_sublist_2)\n",
    "label1 = results[0]\n",
    "label2 = results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and create output file\n",
    "with open(output_dir, \"w\", newline = '',encoding = 'utf-8') as csvfile:\n",
    "    w = csv.writer(csvfile)\n",
    "    w.writerow(['topic_country','sentiment','created_at','title','content'])\n",
    "\n",
    "# Save labels\n",
    "for index, row in df.iterrows():\n",
    "    created_at = row['created_at']\n",
    "    title = row['title']\n",
    "    content = row['content']\n",
    "    topic_country = label1[index]\n",
    "    sentiment = label2[index]\n",
    "    record = [topic_country,sentiment,created_at,title,content]\n",
    "    with open(output_dir, \"a\", newline = '',encoding = 'utf-8') as csvfile:\n",
    "        w = csv.writer(csvfile)\n",
    "        w.writerow(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
