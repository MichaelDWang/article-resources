{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is created by Michael D. Wang linked to our working paper \"Measuring political and economic uncertainty: a supervised computational linguistic approach\".\n",
    "\n",
    "# 1 PUI\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set parameter\n",
    "obs_period = 'monthly'\n",
    "\n",
    "# Set data directory\n",
    "input_dir = \"annotated/valuated_contents.csv\"\n",
    "output_dir = \"data/political uncertainty index/PUI_raw_%s.csv\" %obs_period\n",
    "output_token_dir = \"data/matlab workfile/%s_news_token.csv\" %obs_period\n",
    "output_sort_dir = \"data/political uncertainty index/PUI_%s.csv\" %obs_period\n",
    "\n",
    "# Create output file\n",
    "with open(output_dir, \"w\", newline = '',encoding = 'utf-8') as csvfile:\n",
    "    w = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)\n",
    "    header = ['date','relation_index', 'uncertainty_index', 'keyword_1', 'keyword_2','keyword_3','keyword_4','keyword_5']\n",
    "    w.writerow(header)\n",
    "    \n",
    "with open(output_token_dir, \"w\", newline = '',encoding = 'utf-8') as csvfile:\n",
    "    w = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)\n",
    "    header = ['date','tokens']\n",
    "    w.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def cut(text_str):\n",
    "    # stopwords\n",
    "    stopword_dir = \"Stopwords_Chinese.txt\"\n",
    "    sw_list = []\n",
    "    f = open(stopword_dir, \"r\", encoding='utf-8-sig').read().splitlines()\n",
    "    for word in f:\n",
    "        sw_list.append(word)\n",
    "    text = unicodedata.normalize('NFKC', text_str)\n",
    "    seg_list = jieba.cut(text, cut_all = False, HMM = True)\n",
    "    seg_list = [word.lower() for word in seg_list if word not in sw_list # remove stopwords\n",
    "                and word not in string.punctuation # remove punctuation\n",
    "                and not word.isnumeric() # remove digits\n",
    "                and word not in ['\\ue5e5',' ']] \n",
    "    text1 = \" \".join(seg_list)\n",
    "    return text1.split(\" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Read file\n",
    "df = pd.read_csv(input_dir,encoding='utf-8')#, nrows=10)\n",
    "\n",
    "# Create time series dictionary\n",
    "date_dict = {}\n",
    "l1 = df['created_at'].tolist()\n",
    "\n",
    "# convert date to month\n",
    "if obs_period == 'monthly':\n",
    "    l1 = [datetime.strftime(datetime.strptime(x,'%Y-%m-%d'),'%Y-%m') for x in l1]\n",
    "# convert date to year\n",
    "if obs_period == 'yearly':\n",
    "    l1 = [datetime.strftime(datetime.strptime(x,'%Y-%m-%d'),'%Y') for x in l1]\n",
    "\n",
    "l2 = list(set(l1))\n",
    "l2.sort(key=l1.index)\n",
    "for index,key in enumerate(l2):\n",
    "    date_dict[key]=[]\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    created_at = row['created_at']\n",
    "    decoded_time = datetime.strptime(row['created_at'],'%Y-%m-%d')\n",
    "    if obs_period == 'monthly':\n",
    "        created_at = datetime.strftime(decoded_time,'%Y-%m')\n",
    "    if obs_period == 'yearly':\n",
    "        created_at = datetime.strftime(decoded_time,'%Y')\n",
    "\n",
    "    label1 = row['topic_country']\n",
    "    label2 = row['sentiment']\n",
    "    arousal = row['arousal_degree']\n",
    "    text = row['title'] + row['content']\n",
    "\n",
    "    # ignore irrelevant records\n",
    "    if label1 == 1:\n",
    "        record = [label2, arousal, text]\n",
    "        date_dict[created_at].append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "k = 5\n",
    "j = 1\n",
    "for key in date_dict:\n",
    "    print('Current date: %s'%key)\n",
    "    index1 = 0\n",
    "    distance = 0\n",
    "    text = []\n",
    "    keyword = []\n",
    "\n",
    "    # compute relation index\n",
    "    for record in date_dict[key]:\n",
    "        label2 = record[0]\n",
    "        arousal = record[1]\n",
    "        # add arousal if news is non-negative\n",
    "        if label2 == 1:\n",
    "            index1 += arousal\n",
    "        else:\n",
    "            index1 -= arousal\n",
    "    try:\n",
    "        relation_index = index1/len(date_dict[key])\n",
    "    except ZeroDivisionError:\n",
    "        relation_index = 'nan'\n",
    "    \n",
    "    # compute uncertainty index \n",
    "    for record in date_dict[key]:\n",
    "        arousal = record[1]\n",
    "        # use l2 distance compute volatility of arousal\n",
    "        distance += (arousal - relation_index)**2\n",
    "        text += cut(record[2])\n",
    "    try:\n",
    "        uncertainty_index = distance/len(date_dict[key])\n",
    "    except ZeroDivisionError:\n",
    "        uncertainty_index = 'nan'\n",
    "#     print('Uncertainty index is %.2f'%uncertainty_index)\n",
    "    with open(output_token_dir, \"a\", newline = '',encoding = 'utf-8') as csvfile:\n",
    "        w = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)\n",
    "        output = [key,' '.join(text)]\n",
    "        w.writerow(output)\n",
    "    \n",
    "    # create vocabulary dictionary\n",
    "    word_dict = {}\n",
    "    word_list = list(set(text))\n",
    "    for index,w in enumerate(word_list):\n",
    "        word_dict[w]=1\n",
    "    # count frequency of words in the dictionary\n",
    "    for word in word_dict:\n",
    "        word_dict[word] = text.count(word)\n",
    "    # sort word dictioinary values by frequency and output top k keys\n",
    "    for i in range(0,k):\n",
    "        try:\n",
    "            keyword.append(Counter(word_dict).most_common()[i][0])\n",
    "        except:\n",
    "            keyword = ['nan'] * k\n",
    "    \n",
    "    # save result to output file\n",
    "    with open(output_dir,\"a\",encoding='utf-8') as csvfile:\n",
    "        w = csv.writer(csvfile)\n",
    "        output = [key,relation_index, uncertainty_index, keyword[0], keyword[1], keyword[2], keyword[3], keyword[4]]\n",
    "        w.writerow(output)\n",
    "        \n",
    "    if j%10 == 0:\n",
    "        print('Processing %s out of %s...'%(j,len(date_dict)))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and save sorted result to output file\n",
    "df = pd.read_csv(output_dir)\n",
    "df.sort_values('date', inplace=True,ascending=True)\n",
    "df.to_csv(output_sort_dir, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
